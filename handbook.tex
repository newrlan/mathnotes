% This is a modified version of the tufte-latex book example in which the title page and the contents page resemble Tufte's VDQI book, using Kevin Godby's code from this thread at https://groups.google.com/forum/#!topic/tufte-latex/ujdzrktC1BQ.
%
%% Unfortunately for the contents to contain
%% the "Parts" lines successfully, hyperref
%% needs to be disabled.
\documentclass{tufte-book}
% \usepackage[T2A]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[OT1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage[russian]{babel}
\usepackage{graphicx}

\title{Вероятность и \\ статистика \\ \Huge{Поваренная книга}}

\author{Nurlan Sadykov}

% Абстракт
% Заметки по решению кубика рубика с помощью теории групп.

\begin{document}

\frontmatter


\maketitle  

\part{Probability}

\part{Statistics}

\section{Cookbook}

\begin{enumerate}
    \item \emph{Репозиторий.} Анализ самостоятельного датасета лучше поместить
        в отдельную папку.

    \item \emph{Описальные статистики} стоит просмотреть чтобы понять качество
        данных и исключить сильные аномалии.

    \item Построить \emph{гистограмму} чтобы понять распределение и проверить
        на выбросы.\sidenote{Иногда удобно просматривать логарифмированную
        фичу. Если фича сильно отстоит от нуля, ее лучше переместить в ноль.} 

\end{enumerate}

\chapter{Статистики}

\section{Ящик с усами}

\begin{marginfigure}
    \includegraphics[width=2.7\columnwidth,angle=90]{pics/boxplot.pdf}
    %\caption{Движение $y$}
    \label{fig:boxplot}
\end{marginfigure}

Есть набор наблюдений $x_1, \dots, x_n$, По набору можно вычислить:
\begin{itemize}
    \item $\min$ - минимальное наблюдение
    \item $\max$ - максимальное наблюдение
    \item $M$ - медиана
    \item $Q_1$ - первый квартиль (25\%)
    \item $Q_3$ - третий квартиль (75\%)
\end{itemize}

\emph{Межквартильный размах} задается разностью $\Delta Q = Q_3 - Q_1$.

Длина правого усика, это минимум между расстоянием от квартиля $Q_3$ до
максимального элемента и полутора полуторным межквартильным расстоянием.
Аналогично расчитывается длина правого усика.
\[l = \min(\max(x_i) - Q_3, 1.5 \Delta Q)\]
\[r = \min(Q_1 - \max(x_i), 1.5 \Delta Q)\]
Данные за усиками это \emph{выбросы}, а если они вышли за три межвартильных
расстояния это \emph{чрезвычайные выбросы}.

В ящике с усами используется медиана вместо матожидания\sidenote{Усеченное
    среднее. Если из выборки удалить 2,5\% самых маленьких наблюдений и 2,5\%
    самых больших, то выбросы уже не так сильно будут влиять на вычисленное
матожидание.} в качестве описания потому, что она устойчива к
выбросам.

\chapter{Кластеризация}

\section{Кластерный анализ}
\begin{marginfigure}
    \includegraphics[width=1.1\columnwidth]{pics/dendrogram.pdf}
    %\caption{Движение $y$}
    \label{fig:boxplot}
\end{marginfigure}
Расстояния между кластерами могут быть заданы по разному и стоит обращать
внимание на решаемую задачу. Чтобы задать расстояние между кластерами надо
определить расстояние между точками и на ее основе определить метод построения
кластерного расстояния. Чаще всего используют такие методы:

\begin{itemize}
    \item среднее невзвешенное расстояние (average linkage clustering) -
        среднее расстояние между парами из различных кластеров

    \item центроид метод (устаревший) - расстояние между кластерами
        определяется как расстояние между центрами масс.

    \item метод дальнего соседа (complete linked clustering) - по расстоянию
        между максимально удаленными точками кластеров

    \item метод ближайшего соседа (single linkage clustering) - по расстоянию
        между самыми близкими точками

    \item метод Варда (Ward's method)
\end{itemize}

Дендрограмма выращивается снизу вверх. Сперва каждая точка представляет свой
собственный кластер. Если при достижении уровня $h$ расстояние между точками
равно $h$ то точки объединяются в один кластер (на рисунке это спайка). Процесс
повторяется до тех пор пока не останется один единственный кластер.

Для определения количества кластеров используют метод \emph{плеча}. Для этого
строится график где по оси абсцисс отмечаются шаги объединения (первое
объединение, второе и так далее), по оси ординат откладывается высота на
котором произошло объединение кластеров. На графике ищется точка перелома,
когда расстояние резко увеличивается. Это количество кластеров в данных по
методу локтя.
\sidenote{Результаты кластерного анализа нужно интерпретировать: анализ всегда
должен давать что-то новое о данных, что общего у объектов в кластере и чем
различаются кластеры.}
\sidenote[][1\baselineskip]{Кривую локтя можно построить и для метода k-средних. В этом случае по
оси абсцисс будет лежать выбранный $k$ по оси ординат качество получившейся
кластеризации {\color{red} TODO добавить линк} }

Недостатки иерархической кластеризации:
\begin{itemize}
    \item Плохо справляется с данными которые вытянуты в длинные ленты в
        простраснтве

    \item Для вычисления требуется хранить попарные расстояния между объектами.
\end{itemize}

\section{DBSCAN}
Density-based spatial clustering of applications with noise. Сильный
топологический алгоритм. Основан на поиске связных компонент в покрытии данных
$\epsilon$-шарами. Количество кластеров определяет автоматически.

Все объекты в наблюдениях делятся на три категории:
\begin{itemize}
    \item core point (внутренние / основные) - если в окрестности есть более
        $N_0$ соседей

    \item border point (граничные) - если в окрестности меньше $N_0$ внутренних

    \item noise point (шумовые) - если в окрестности нет внутренних точек.
        Автоматически содержат меньше $N_0$ объектов.
\end{itemize}

Алгоритм DBSCAN:
\begin{marginfigure}
    \includegraphics[width=1.3\columnwidth]{pics/dbscan.jpeg}
    %\caption{Движение $y$}
    \label{fig:boxplot}
\end{marginfigure}
\begin{enumerate}
    \item Шумовые точки удаляются из рассмотрения и не приписываются ни какому
        кластеру.

    \item Основные точки у которых есть общая окрестность соединяются ребром.
        Строится граф.

    \item В полученном графе вычисляются компоненты связности.

    \item Каждая граничная точка относится к тому кластеру, в который попала
        ближайшая к ней внутренняя точка.
\end{enumerate}

\chapter{Статистические гипотезы}

\section{Ошибки, значимость, p-value}

Если мы хотим опровергнуть какое-то утверждение, мы должны сформулировать
альтернативную гипотезу. Основная гипотеза всегда обозначается $H_0$,
альтернативная $H_1$. Основная гипотеза всегда должна быть простой, каким-то
конкретным утверждением, альтернативная может быть каким угодно. Такое
требование создается потому, что у нас нет мат.аппарата на сложные условия.

Если мы хотим проверить гипотезу, мы можем совершить такие ошибки:
\begin{table}
    \caption{Ошибки I-го и II-го рода}
    \label{tab:mistakes}
    \begin{center}
        \begin{tabular}[c]{c|c c}
             & гипотеза верна & гипотеза не верна \\
            % \multicolumn{1}{c|}{\textbf{}} & 
            % \multicolumn{1}{c|}{\textbf{гипотеза верна}} & 
            % \multicolumn{1}{c}{\textbf{гипотеза не верна}} \\
            \hline
            гипотеза принята & ok & \textbf{ошибка II рода} \\
            гипотеза отвергнута & {\it ошибка I рода} & ok \\
            
            % \hline
        \end{tabular}
    \end{center}
\end{table}

Влиять мы можем только на ошибку I-го рода. Ее обычно фиксируют на уровне 0,05,
0,01 или 0,005, но этот уровень можно менять на любой другой если этого требует
задача.

Ошибки второго рода контролировать сложнее. Для их уменьшения стараются
пользоваться состоятельными критериями.\sidenote{Критерий проверки гипотезы
называется состоятельным, если ошибка второго рода уменьшается с ростом числа
наблюдений.}

\begin{marginfigure}
    \includegraphics[width=1.1\columnwidth]{pics/critical_points_mistakes_value.pdf}
    \caption{Левое распределение соответствует гипотезе $H_0$, правое - гипотезе $H_1$.}
    \label{fig:crit_point}
\end{marginfigure}
Критическая точка критерия это такое значение вероятность превысить которое
равняется~$\alpha$ то есть вероятности ошибки первого рода. На рисунке
\ref{fig:crit_point} это площадь под графиком выше критической точки окрашенная
в красный. Синим обозначена вероятность ошибки II-го рода. Как видно это
вероятность получить выборку при которой не отвергается нулевая гипотеза, но
была верна альтернативная гипотеза.\sidenote[][1\baselineskip]{Проверять
    статистические гипотезы можно только в случае массовых явлений. События
    которые редки стат.гипотезами не проверяются. Число в 30 наблюдений
    считается достаточным только для нормальных распределений, для всех
    остальных надо больше данных.}

Критическая точка не обязана лежать на пересечении плотностей распределений
двух гипотез. Она определяется исходя только из плотности для нулевой гипотезы.

Альтернативой к механике проверки гипотез с критической точкой является
проверка гипотез через p-value. P-value вычисляется из предположения, что
нулевая гипотеза верна и определяет вероятность получить такое же значение
критерия или более экстремальное. Нам все равно нужно фиксировать уровень
значимости~$\alpha$ до эксперимента, но не нужно вычислять критическую точку.
Если p-value оказалось меньше~$\alpha$ то нулевая гипотеза должна быть
отвергнута, в противном случае нет оснований отвергать нулевую гипотезу.

\section{Нормальность}
Проверять данные на нормальность стоит не слишком тщательно. Данным достаточно только \emph{напоминать} нормальные, чтобы использовать большинство критериев в которых подразумевается, что данные нормальные. Если подходить к проверкам строго, то ничто и никогда невозможно будет проверить. 

Точно необходимо проверить:
\begin{enumerate}
    \item Выбросы. Отсутствие выбросов достаточно критичное условие. Если в данных есть выбросы, то их нельзя считать нормальными. С другой стороны если эти выбросы убрать, возможно уже можно будет работать.

    \item Асимметрия. Нормальные данные должны быть симметричными, поэтому если наблюдается асимметрия, то данные не нормальные. Возможно сможет помочь логарифмирование. 

    \item Отклонение от колоколообразности. Пограничным случаем колоколообразности можно считать равномерное распределение, а вот бимодальность гистограммы уже явный признак, что данные не нормальны. Еще чем больше наблюдений в данных, тем все-таки ближе должна быть гистограмма к нормальному распределению. За критерий можно взять выборку в $n=150$, чей график должен быть похож на колокол.
\end{enumerate}

Если в выборке менее 2000 объектов, то применяется метод Шапиро-Уилка, если же объектов больше, то используется критерий согласия Колмогорова-Смирнова.

\section{Сравнение мат.ожиданий}
Для сравнения мат.ожиданий используются:
\begin{itemize}
    \item t-критерий Стюдента (важно обращать внимание на равенство дисперсий)
    \item Fligner-Killeen test
    \item Brown-Forsythe test
\end{itemize}

\section{Сравнение медиан}
Для сравнения медиан используются:
\begin{itemize}
    \item Mood's median test. Только для гигантских данных. На малых данных будет большая ошибка 2 рода.
    \item критерий Мана-Уитни. Есть критика критерия, но крайние случаи довольно редкие и ими пренебрегают и все равно используют критерий.
\end{itemize}

\section{Проверка зависимости}
При проверке зависимости между двумя случайными величинами используют коэффициенты корреляции:
\begin{itemize}
    \item Пирсона, если данные нормальные.
    \item Спирмена, в остальных случаях.
    \item Кендела, очень редко.
\end{itemize}

При вычислении корреляций надо понимать, что они ищут только линейные зависимости, коэффициенты могут быть очень чувствительными к выбросам и иногда могут показывать ложную корреляцию, например, для монотонных временных рядов.

\end{document}
