% This is a modified version of the tufte-latex book example in which the title page and the contents page resemble Tufte's VDQI book, using Kevin Godby's code from this thread at https://groups.google.com/forum/#!topic/tufte-latex/ujdzrktC1BQ.
%
%% Unfortunately for the contents to contain
%% the "Parts" lines successfully, hyperref
%% needs to be disabled.
\documentclass[a4paper, oneside]{book}


\usepackage{amsmath, systeme, amsfonts}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% -- Настройка боковых заметок ------------------------------------------------
\usepackage[a4paper, marginparwidth=6cm, marginparsep=0.8cm]{geometry}
\geometry{left=1.5cm}
\geometry{right=8cm}

\setlength{\marginparpush}{15pt}


\usepackage[T2A]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage[russian]{babel}
\usepackage{graphicx}

% marginnote создает НЕ нумерованные плавающие боковые заметки
\usepackage{marginnote}

% Здесь делаем маленький шрифт для marginnote
\let\oldmarginpar\marginpar
\renewcommand\marginpar[1]{\-\oldmarginpar[\raggedleft\footnotesize #1]%
{\raggedright\footnotesize #1}}

% sidenotes создает НУМЕРОВАННЫЕ плавающие боковые заметки и боковые картинки
\usepackage{sidenotes}
% Здесь делаем маленький шрифт для sidenotes
\makeatletter
\RenewDocumentCommand\sidenotetext{ o o +m }{%      
    \IfNoValueOrEmptyTF{#1}{%
        \@sidenotes@placemarginal{#2}{\textsuperscript{\thesidenote}{}~\footnotesize#3}%
        \refstepcounter{sidenote}%
    }{%
        \@sidenotes@placemarginal{#2}{\textsuperscript{#1}~#3}%
    }%
}
\makeatother
% -----------------------------------------------------------------------------

% -- Настройка библиографии ---------------------------------------------------
\usepackage[backend=biber]{biblatex}
\addbibresource{biblio.bib}
% -----------------------------------------------------------------------------

% % -- Настройка форматирование глав и секций -----------------------------------
% \usepackage{titlesec}
% 
% \titleformat
% {\chapter} % command
% [display] % shape
% {\bfseries\Large\itshape} % format
% {Story No. \ \thechapter} % label
% {0.5ex} % sep
% {
%     \rule{\textwidth}{1pt}
%     \vspace{1ex}
%     \centering
% } % before-code
% [
% \vspace{-0.5ex}%
% \rule{\textwidth}{0.3pt}
% ] % after-code
% 
% 
% \titleformat{\section}[wrap]
% {\normalfont\bfseries}
% {\thesection.}{0.5em}{}
% 
% \titlespacing{\section}{12pc}{1.5ex plus .1ex minus .2ex}{1pc}
% % -----------------------------------------------------------------------------

\usepackage[breaklinks,hidelinks]{hyperref}


\title{Вероятность и \\ статистика \\ \Huge{Поваренная книга}}

\author{Nurlan Sadykov}

% Абстракт
% Заметки по решению кубика рубика с помощью теории групп.

\begin{document}

\maketitle  
\tableofcontents


\part{Probability}

\part{Statistics}

\section{Cookbook}

\begin{enumerate}
    \item \emph{Репозиторий.} Анализ самостоятельного датасета лучше поместить
        в отдельную папку.

    \item \emph{Описальные статистики} стоит просмотреть чтобы понять качество
        данных и исключить сильные аномалии.

    \item Построить \emph{гистограмму} чтобы понять распределение и проверить
        на выбросы.\marginpar{Иногда удобно просматривать логарифмированную
        фичу. Если фича сильно отстоит от нуля, ее лучше переместить в ноль.}

\end{enumerate}

\section{Train, Test, Validation}
\begin{marginfigure}
    \includegraphics[width=1\columnwidth]{pics/choise_of_model.pdf}
    \caption{\emph{Выбор модели:} по оси абсцисс лежат модели (например, это могут быть степени аппроксимирующих многочленов), по оси ординат отмечается ошибка. Выбирается та модель у которой достигается наименьшая ошибка на тестовой выборке (на рисунке точка opt).}
    \label{fig:choise_of_model}
\end{marginfigure}
Общая рекомендация относить в тестовую выборку до 30\% всех данных. Методику с train-test'ом используют чтобы выбрать модель которой. Например, в задаче регрессии нужно понять многочленами какой степени стоит аппроксимировать данные. Отбирается та модель которая на тестовой выборке показывает минимальную ошибку. В цлеом это так или иначе упирается в кросс-валидацию.

\begin{itemize}
    \setlength\itemsep{0em}
    \item Обучащающая выборка - для обучения модели
    \item Тестовая выборка - для замера качества модели
    \item Валидационная выборка - для подбора гиппер-параметров
\end{itemize}

Альтернативой к валидации является подход с регуляризацией.

\section{Ящик с усами}

\begin{marginfigure}
    \includegraphics[width=2.7\columnwidth,angle=90]{pics/boxplot.pdf}
    %\caption{Движение $y$}
    \label{fig:boxplot}
\end{marginfigure}

Есть набор наблюдений $x_1, \dots, x_n$, По набору можно вычислить:
\begin{itemize}
    \item $\min$ - минимальное наблюдение
    \item $\max$ - максимальное наблюдение
    \item $M$ - медиана
    \item $Q_1$ - первый квартиль (25\%)
    \item $Q_3$ - третий квартиль (75\%)
\end{itemize}

\emph{Межквартильный размах} задается разностью $\Delta Q = Q_3 - Q_1$.

Длина правого усика, это минимум между расстоянием от квартиля $Q_3$ до
максимального элемента и полутора полуторным межквартильным расстоянием.
Аналогично расчитывается длина правого усика.
\[l = \min(\max(x_i) - Q_3, 1.5 \Delta Q)\]
\[r = \min(Q_1 - \max(x_i), 1.5 \Delta Q)\]
Данные за усиками это \emph{выбросы}, а если они вышли за три межвартильных
расстояния это \emph{чрезвычайные выбросы}.

В ящике с усами используется медиана вместо матожидания\marginpar{Усеченное
    среднее. Если из выборки удалить 2,5\% самых маленьких наблюдений и 2,5\%
    самых больших, то выбросы уже не так сильно будут влиять на вычисленное
матожидание.} в качестве описания потому, что она устойчива к
выбросам.

\chapter{Статистические гипотезы}

\section{Ошибки, значимость, p-value}

\begin{margintable}
    \caption{Ошибки I-го и II-го рода}
    \label{tab:mistakes}
    \begin{center}
        \begin{tabular}[c]{c|c c}
            {\it гипотеза} & верна & не верна \\
            \hline
            принята & + & {\it II род} \\
            отвергнута & {\it I род} & + \\
        \end{tabular}
    \end{center}
\end{margintable}

Если мы хотим опровергнуть какое-то утверждение, мы должны сформулировать
альтернативную гипотезу. Основная гипотеза всегда обозначается $H_0$,
альтернативная $H_1$. Основная гипотеза всегда должна быть простой, каким-то
конкретным утверждением, альтернативная может быть каким угодно. Такое
требование создается потому, что у нас нет мат.аппарата на сложные условия.

Влиять мы можем только на ошибку I-го рода. Ее обычно фиксируют на уровне 0,05,
0,01 или 0,005, но этот уровень можно менять на любой другой если этого требует
задача.

Ошибки второго рода контролировать сложнее. Для их уменьшения стараются
пользоваться состоятельными критериями.\marginpar{Критерий проверки гипотезы
называется состоятельным, если ошибка второго рода уменьшается с ростом числа
наблюдений.}

\begin{marginfigure}
    \includegraphics[width=1.1\columnwidth]{pics/critical_points_mistakes_value.pdf}
    \caption{Левое распределение соответствует гипотезе $H_0$, правое - гипотезе $H_1$.}
    \label{fig:crit_point}
\end{marginfigure}
Критическая точка критерия это такое значение вероятность превысить которое
равняется~$\alpha$ то есть вероятности ошибки первого рода. На рисунке
\ref{fig:crit_point} это площадь под графиком выше критической точки окрашенная
в красный. Синим обозначена вероятность ошибки II-го рода. Как видно это
вероятность получить выборку при которой не отвергается нулевая гипотеза, но
была верна альтернативная гипотеза.\marginpar{Проверять
    статистические гипотезы можно только в случае массовых явлений. События
    которые редки стат.гипотезами не проверяются. Число в 30 наблюдений
    считается достаточным только для нормальных распределений, для всех
    остальных надо больше данных.}

Критическая точка не обязана лежать на пересечении плотностей распределений
двух гипотез. Она определяется исходя только из плотности для нулевой гипотезы.

Альтернативой к механике проверки гипотез с критической точкой является
проверка гипотез через p-value. P-value вычисляется из предположения, что
нулевая гипотеза верна и определяет вероятность получить такое же значение
критерия или более экстремальное. Нам все равно нужно фиксировать уровень
значимости~$\alpha$ до эксперимента, но не нужно вычислять критическую точку.
Если p-value оказалось меньше~$\alpha$ то нулевая гипотеза должна быть
отвергнута, в противном случае нет оснований отвергать нулевую гипотезу.

\subsection{Ошибка подглядывания}
Ошибка подглядывания возникает когда мы заглядываем в результаты теста до
окончания самого теста (в промежуточное время). В этот момент человек может
увидеть промежуточный результат p-value~$< \alpha$. Во время подглядывания
может возникнуть соблазн дождаться когда p-value упадет ниже уровня
стат.значимости и остановить эксперимент. На самом деле график p-value в
процессе эксперимента может несколько раз опускаться ниже стат.значимости. Это
будет происходить случайно.\sidenote{\fullcite{karpov:ab_peeping}}

\marginpar{{\bf Процедура статистического тестирования:}
    \begin{enumerate}
        \item Определить метрику которую проверяем.
        \item Сформулировать нулевую и альтернативную гипотезы.
        \item Зафиксировать уровень стат.значимости~$\alpha$.
        \item Предрасчитать объем необходимой выборки для теста.
        \item Предрасчитать длительность теста.
        \item Запуск теста. В промежуточные результаты не заглядываем.
        \item По истечению времени, интерпретация результатов.
    \end{enumerate}

    Такая процедура обеспечит, что из всей массы запущенных экспериментов ошибки I рода будут наблюдаться с вероятностью~$\alpha$.
}

В случае если \emph{нулевая гипотеза верна}, вероятность получить p-value на
определенном уровне имеет равномерное распределение. Например, вероятность
получить p-value из первого дециля равна 10\%, вероятность получить p-value из
второго дециля это $P(\text{p-value} < 0.2) - P(\text{p-value} < 0.1) = 0.1$
тоже 10\%. Это означает, что отвергать нулевую гипотезу, то есть совершать ошибку I рода, мы будем с вероятностью~$\alpha$. 

Если же следить за p-value пока он не опустится ниже~$\alpha$ мы увеличим вероятность ошибки I рода и перестанем ее контролировать. И по сути будем выдавать желаемое за действительное.

\section{bootstrap}
Довольно мощный тест который позволяет вычислить параметры распределения или
какие-то дополнительные метрики по выборке.\marginpar{
    \it Bootstrap предполагает, что начальная выборка из которой мы производим
    сэмплирования является \emph{репререзентативной}.
}

Положим, что у нас какая-то \emph{репрезентативная} выборка~$S$ и мы хотим
оценить некоторый параметр~$\nu$. Например, это может быть первый квартиль или
разность матожиданий. 

Чтобы провести bootstrap проведем~$X(> 1000)$ раз процедуру:
\begin{enumerate}
    \item Просэмплируем из множества~$S$ подвыборку~$N$ элементов с возвращениями. На практике достаточно $50 < N < 100$.
    \item Посчитаем на полученной подвыборке параметр~$\nu$.
\end{enumerate}
В итоге мы получаем новую выборку из $X$ оценок параметра~$\nu$ и теперь можем оценить и как матожидание этого параметра, так и его доверительный интервал.

\section{Нормальность}
Проверять данные на нормальность стоит не слишком тщательно. Данным достаточно только \emph{напоминать} нормальные, чтобы использовать большинство критериев в которых подразумевается, что данные нормальные. Если подходить к проверкам строго, то ничто и никогда невозможно будет проверить. 

Точно необходимо проверить:
\begin{enumerate}
    \item Выбросы. Отсутствие выбросов достаточно критичное условие. Если в данных есть выбросы, то их нельзя считать нормальными. С другой стороны если эти выбросы убрать, возможно уже можно будет работать.

    \item Асимметрия. Нормальные данные должны быть симметричными, поэтому если наблюдается асимметрия, то данные не нормальные. Возможно сможет помочь логарифмирование. 

    \item Отклонение от колоколообразности. Пограничным случаем колоколообразности можно считать равномерное распределение, а вот бимодальность гистограммы уже явный признак, что данные не нормальны. Еще чем больше наблюдений в данных, тем все-таки ближе должна быть гистограмма к нормальному распределению. За критерий можно взять выборку в $n=150$, чей график должен быть похож на колокол.
\end{enumerate}

Если в выборке менее 2000 объектов, то применяется метод Шапиро-Уилка, если же объектов больше, то используется критерий согласия Колмогорова-Смирнова.

\section{Сравнение мат.ожиданий}
Для сравнения мат.ожиданий используются:
\begin{itemize}
    \item t-критерий Стюдента (важно обращать внимание на равенство дисперсий)
    \item Fligner-Killeen test
    \item Brown-Forsythe test
\end{itemize}

\subsection{t-test, критерий Стьюдента}
Критерий Стюдента предполагает что метрика (н-р, матожидание) нормально распределена. Для двухвыборочного критерия важно равенство дисперсий. Так же тест не устойчив к выбросам в данных.

Одновыборочный тест\marginpar{
    Статистика для одновыборочного теста:\\
    $\overline x$ - выборочное матожидание \\
    $s^2$ - выборочная несмещенная оценка дисперсии \\
    $n$ - объем выборки
    \[H_0: M(x) = m\]
    \[t = \frac{\overline{x} - m}{s / \sqrt{n}}\]
    Степеней свободы $n-1$.

}
проводится когда у нас есть точное предположение о значении матожидания.

\paragraph{Двухвыборочный тест}
Предполагается, что есть две независимые выборки объема~$n_1$ и~$n_2$. Из
которых извлечены матожидания~$m_1$ и~$m_2$ и несмещенные дисперсии~$s_1^2$
и~$s_2^2$. 

\[H_0:m_1 = m_2\]

Если нулевая гипотеза верна, то разность $\Delta = m_1 - m_2$ имеет матожидание
$M(\Delta) = 0$\marginpar{
    Превращаем двувыборочный тест в одновыборочный!
}
и исходя из независимости выборок $D(\Delta) =
\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}$, так же получаем несмещенную
оценку разности выборочных средних $D(\Delta) = \frac{s_1^2}{n_1} +
\frac{s_2^2}{n_2}$. Теперь можем определить статистику

\begin{equation}
    \label{eq:t-test}
    t = \frac{m_1 - m_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
\end{equation}

Степени свободы вычисляются по формуле
\begin{equation}
    \label{eq:t-test-degree}
    df = \frac{
        \left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\right)^2
    }{
        (\frac{s_1^2}{n_1})^2/(n_1 - 1) + (\frac{s_2^2}{n_2})^2/(n_2-1)
    }
\end{equation}

\section{Сравнение медиан}
Для сравнения медиан используются:
\begin{itemize}
    \item Mood's median test. Только для гигантских данных. На малых данных будет большая ошибка 2 рода.
    \item критерий Мана-Уитни. Есть критика критерия, но крайние случаи довольно редкие и ими пренебрегают и все равно используют критерий.
\end{itemize}

\section{Проверка зависимости}
При проверке зависимости между двумя случайными величинами используют коэффициенты корреляции:
\begin{itemize}
    \setlength\itemsep{0em}
    \item Пирсона, если данные нормальные.
    \item Спирмена, в остальных случаях.
    \item Кендела, очень редко.
\end{itemize}

При вычислении корреляций надо понимать, что они ищут только линейные зависимости, коэффициенты могут быть очень чувствительными к выбросам и иногда могут показывать ложную корреляцию, например, для монотонных временных рядов.

\chapter{Кластеризация}

\section{Кластерный анализ}
\begin{marginfigure}
    \includegraphics[width=1.1\columnwidth]{pics/dendrogram.pdf}
    %\caption{Движение $y$}
    \label{fig:boxplot}
\end{marginfigure}
Расстояния между кластерами могут быть заданы по разному и стоит обращать
внимание на решаемую задачу. Чтобы задать расстояние между кластерами надо
определить расстояние между точками и на ее основе определить метод построения
кластерного расстояния. Чаще всего используют такие методы:

\begin{itemize}
    \item среднее невзвешенное расстояние (average linkage clustering) -
        среднее расстояние между парами из различных кластеров

    \item центроид метод (устаревший) - расстояние между кластерами
        определяется как расстояние между центрами масс.

    \item метод дальнего соседа (complete linked clustering) - по расстоянию
        между максимально удаленными точками кластеров

    \item метод ближайшего соседа (single linkage clustering) - по расстоянию
        между самыми близкими точками

    \item метод Варда (Ward's method)
\end{itemize}

Дендрограмма выращивается снизу вверх. Сперва каждая точка представляет свой
собственный кластер. Если при достижении уровня $h$ расстояние между точками
равно $h$ то точки объединяются в один кластер (на рисунке это спайка). Процесс
повторяется до тех пор пока не останется один единственный кластер.

Для определения количества кластеров используют метод \emph{плеча}. Для этого
строится график где по оси абсцисс отмечаются шаги объединения (первое
объединение, второе и так далее), по оси ординат откладывается высота на
котором произошло объединение кластеров. На графике ищется точка перелома,
когда расстояние резко увеличивается. Это количество кластеров в данных по
методу локтя.
\marginpar{Результаты кластерного анализа нужно интерпретировать: анализ всегда
должен давать что-то новое о данных, что общего у объектов в кластере и чем
различаются кластеры.}
\marginpar{
    Кривую локтя можно построить и для метода k-средних. В этом случае по оси
    абсцисс будет лежать выбранный $k$ по оси ординат качество получившейся
    кластеризации.

    См. \fullcite{yandex:book}, глава 7.
}

Недостатки иерархической кластеризации:
\begin{itemize}
    \item Плохо справляется с данными которые вытянуты в длинные ленты в
        простраснтве

    \item Для вычисления требуется хранить попарные расстояния между объектами.
\end{itemize}

\section{DBSCAN}
Density-based spatial clustering of applications with noise. Сильный
топологический алгоритм. Основан на поиске связных компонент в покрытии данных
$\epsilon$-шарами. Количество кластеров определяет автоматически.

Все объекты в наблюдениях делятся на три категории:
\begin{itemize}
    \item core point (внутренние / основные) - если в окрестности есть более
        $N_0$ соседей

    \item border point (граничные) - если в окрестности меньше $N_0$ внутренних

    \item noise point (шумовые) - если в окрестности нет внутренних точек.
        Автоматически содержат меньше $N_0$ объектов.
\end{itemize}

Алгоритм DBSCAN:
\begin{marginfigure}
    \includegraphics[width=1.3\columnwidth]{pics/dbscan.jpeg}
    %\caption{Движение $y$}
    \label{fig:boxplot}
\end{marginfigure}
\begin{enumerate}
    \item Шумовые точки удаляются из рассмотрения и не приписываются ни какому
        кластеру.

    \item Основные точки у которых есть общая окрестность соединяются ребром.
        Строится граф.

    \item В полученном графе вычисляются компоненты связности.

    \item Каждая граничная точка относится к тому кластеру, в который попала
        ближайшая к ней внутренняя точка.
\end{enumerate}

\chapter{Классификация и регрессия}

\section{Линейная регрессия}
\begin{marginfigure}
    \includegraphics[width=1\columnwidth]{pics/anscombe_quartet.pdf}
    \caption{Квартет Анскомба. Иллюстрация как линейная регрессия может лажать. Для данных подобрали не ту модель или в данных есть сильные выбросы - все приводит к некорректному построению регрессии.}
    \label{fig:anscombe}
\end{marginfigure}

Для Определения на сколько модель хороша используют коэффициент детерминации. Чтобы его вычислить нужна обученная модель.
\begin{equation}
    \label{eq:coeff_of_determination}
    R^2 = 1 - \frac{D[y|x]}{D[y]}
\end{equation}
В сущностной части это на сколько дисперсия предсказания отличается от дисперсии наблюдений.

\marginpar{
    \emph{Наивная мультивариантная линейная регрессия:}
    \[ y = X\beta + \epsilon\]
    \[\epsilon \sim N(0,\, \sigma^2 I)\]
    $\epsilon$ - коэффициент \emph{невязки}, определяет шум системы.

    Коэффициенты находятся формулой
    \[ \hat \beta = (X^TX)^{-1}X^Ty \]

    Вариация вычисленных коэффициентов
    \[ Var(\hat \beta) = \sigma^2 (X^TX)^{-1}\]
    Это легко вывести если воспользоваться формулой дисперсии произведения 
    \[Var(AX) = A \cdot Var(X) \cdot A^T.\]
}

Во время построения модели регрессии самая большая проблема - наличие коллинеарных векторов (зависимых переменных в фичах). Чтобы очистить список фичей, можно проверить гипотезу что вычисленная фича нулевая (для каждой фичи индивидуально). 

\begin{equation*}
    \label{eq:zero_coeff_hyp}
    \left\{
    \begin{array}{lr}
        H_0: & \; \hat \beta_i = 0 \\
        H_1: & \; \hat \beta_i \ne 0
    \end{array}
    \right.
\end{equation*}

Если предположить что коэффициент имеет нормальное распределение, то мы можем
воспользоваться t-тестом Стьюдента и посчитать p-value. Если гипотеза не будет
отвергнута, это будет означать, что мы фичу стоит выкинуть потому что ее
коэффициент равен нулю. Соответственно там где гипотеза не подтвердилась фичи
являются ценными.

Выкидывать фичи стоит по одному. Начиная с самой слабой по p-value, каждый раз
с новым набором фичей анализ лучше проводить заново.\sidenote{\fullcite{Watts:ItS}}

\subsection{Для временных рядов}
Линейная регрессия может быть очень полезной во временных рядах, когда мы наблюдаем в них несколько сезонностей или когда данных относительно мало, нет нескольких полных сезонов в данных.

Чтобы учесть сезонность во временных рядах создают фичи:
\begin{itemize}
    \setlength\itemsep{0em}
    \item 1 - свободный коэффициент, bias
    \item $\delta_s(t)$ - учет сезонности, бинарный признак, что дата/время $t$ попало во временной интервал~$s$  (час, день, неделя, месяц и пр).
\end{itemize}
При этом очевидно, что если сложить все фичи $\delta_s(t)$ сезонности мы получим свободную переменную, то есть на лицо линейная зависимость. 

\section{Деревяшки (решающие деревья)}

\marginpar{
    {\bf Обозначения:}
    \begin{description}
        \item $N$ --- объем выборки
        \item $D$ --- размерность признаков
        \item $y = \{ y_i \}_{i=1}^N \subset \mathbb{R}^N$ --- вектор таргетов
        \item $X = \{x_i\}_{i=1}^N \in \mathbb{R}^{N \times D}$ --- матрица признаков
    \end{description}

    %\[x_{\text{строка}}^{\text{столбец}}\]
}

Решающее дерево имеет в узлах предикаты, а в листьях метки классов или другие значения в зависимости от задачи.

Чаще всего строят бинарные решающие деревья и используют такие предикаты:
\begin{equation}
    \label{eq:tree_predicat}
    B(x, j, t) = [x^j \le t] = B_{j,t}(x)
\end{equation}

\marginpar{
    \emph{Impurity criterion (критерий информативности)}~$H(R)$ оценивает
    качество распределения целевой переменной в множестве~$R$. 

    Поскольку это про оценку распределения часто используют энтропию или индекс
    Джини, но также может использоваться и квадрат/модуль ошибок и прочее.
}

Если в вершине выбрано какое-то разбиение и множество $R$ было разбито на два $R_l$ и $R_r$ то можно почитать на сколько мы сделали лучше этим разбиением.\sidenote{\fullcite{sokolov:desigion_tree}}
\begin{equation}
    \label{eq:tree_split_quality}
    Q(R, B_{j,t}) = H(R) - \frac{| R_l |}{| R | } H(R_l) - \frac{| R_r |}{| R |}H(R_r)
\end{equation}


Чтобы выбрать лучший вариант разбиения условия разбиения по признаку~$j$ можно
упорядочить все наблюдения в узле по этому признаку постепенно перебрать все
варианты разбиений и выбрать тот в котором функция качества~$Q$ достигает
максимума. Эту процедуру повторяем для всех $j = \overline{1, D}$ и выбираем лучшее разбиение.

\begin{equation}
    \label{eq:opt_node_value}
    (j_{opt}, t_{opt}) = \argmax_{j, t} Q(R, B_{j,t})
\end{equation}

\marginpar{Если на тестовой выборке результат заметно хуже, а мы очень хотим обучить дерево, можно попробовать упростить модель, сильнее ограничить глубину дерева.}

\marginpar{
    Для решения \emph{задачи регрессии} достаточно подобрать критерий
    информативности. Например, это может быть квадрат отклонения от среднего

    \[
        H(R) = \frac{1}{|R|} \sum_{(x_i,\,y_i) \in R}
        \left(
            y_i - \frac{1}{|R|} \sum_{(x_i,\,y_i) \in R} y_i
        \right)^2
    \]
}

Для процедуры критерий останова можно вводить разными способами. Самые
популярные такие:
\begin{itemize}
    \setlength\itemsep{0em}
    \item максимальная глубина дерева,
    \item максимальное количество листьев,
    \item минимальное число объектов в листе,
    \item все объекты в листе лежат в одном классе,
    \item качество улучшилось не более чем на $x$ процентов.
\end{itemize}

Деревья дают кусочно-линейные решения, однако у них есть преимущества в том,
что они могут вытащить нелинейные зависимости; а также и недостатки, за
пределами наблюдений регрессия будет предсказываться константой. Другими
словами регрессия на деревьях не умеет экстраполировать, зато может быть
полезна в интерполяции.

\medskip
\printbibliography

\end{document}
