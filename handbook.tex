% This is a modified version of the tufte-latex book example in which the title page and the contents page resemble Tufte's VDQI book, using Kevin Godby's code from this thread at https://groups.google.com/forum/#!topic/tufte-latex/ujdzrktC1BQ.
%
%% Unfortunately for the contents to contain
%% the "Parts" lines successfully, hyperref
%% needs to be disabled.
\documentclass[a4paper, oneside]{book}


\usepackage{amsmath, systeme}

% -- Настройка боковых заметок ------------------------------------------------
\usepackage[a4paper, marginparwidth=6cm, marginparsep=0.8cm]{geometry}
\geometry{left=1.5cm}
\geometry{right=8cm}

\setlength{\marginparpush}{15pt}


\usepackage[T2A]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage[russian]{babel}
\usepackage{graphicx}

% marginnote создает НЕ нумерованные плавающие боковые заметки
\usepackage{marginnote}

% Здесь делаем маленький шрифт для marginnote
\let\oldmarginpar\marginpar
\renewcommand\marginpar[1]{\-\oldmarginpar[\raggedleft\footnotesize #1]%
{\raggedright\footnotesize #1}}

% sidenotes создает НУМЕРОВАННЫЕ плавающие боковые заметки и боковые картинки
\usepackage{sidenotes}
% Здесь делаем маленький шрифт для sidenotes
\makeatletter
\RenewDocumentCommand\sidenotetext{ o o +m }{%      
    \IfNoValueOrEmptyTF{#1}{%
        \@sidenotes@placemarginal{#2}{\textsuperscript{\thesidenote}{}~\footnotesize#3}%
        \refstepcounter{sidenote}%
    }{%
        \@sidenotes@placemarginal{#2}{\textsuperscript{#1}~#3}%
    }%
}
\makeatother
% -----------------------------------------------------------------------------

% -- Настройка библиографии ---------------------------------------------------
\usepackage[backend=biber]{biblatex}
\addbibresource{biblio.bib}
% -----------------------------------------------------------------------------

\usepackage[breaklinks,hidelinks]{hyperref}


\title{Вероятность и \\ статистика \\ \Huge{Поваренная книга}}

\author{Nurlan Sadykov}

% Абстракт
% Заметки по решению кубика рубика с помощью теории групп.

\begin{document}

\frontmatter


\maketitle  

\part{Probability}

\part{Statistics}

\section{Cookbook}

\begin{enumerate}
    \item \emph{Репозиторий.} Анализ самостоятельного датасета лучше поместить
        в отдельную папку.

    \item \emph{Описальные статистики} стоит просмотреть чтобы понять качество
        данных и исключить сильные аномалии.

    \item Построить \emph{гистограмму} чтобы понять распределение и проверить
        на выбросы.\marginpar{Иногда удобно просматривать логарифмированную
        фичу. Если фича сильно отстоит от нуля, ее лучше переместить в ноль.}

\end{enumerate}

\section{Train, Test, Validation}
\begin{marginfigure}
    \includegraphics[width=1\columnwidth]{pics/choise_of_model.pdf}
    \caption{\emph{Выбор модели:} по оси абсцисс лежат модели (например, это могут быть степени аппроксимирующих многочленов), по оси ординат отмечается ошибка. Выбирается та модель у которой достигается наименьшая ошибка на тестовой выборке (на рисунке точка opt).}
    \label{fig:choise_of_model}
\end{marginfigure}
Общая рекомендация относить в тестовую выборку до 30\% всех данных. Методику с train-test'ом используют чтобы выбрать модель которой. Например, в задаче регрессии нужно понять многочленами какой степени стоит аппроксимировать данные. Отбирается та модель которая на тестовой выборке показывает минимальную ошибку. В цлеом это так или иначе упирается в кросс-валидацию.

Альтернативой к валидации является подход с регуляризацией.

\chapter{Статистики}

\section{Ящик с усами}

\begin{marginfigure}
    \includegraphics[width=2.7\columnwidth,angle=90]{pics/boxplot.pdf}
    %\caption{Движение $y$}
    \label{fig:boxplot}
\end{marginfigure}

Есть набор наблюдений $x_1, \dots, x_n$, По набору можно вычислить:
\begin{itemize}
    \item $\min$ - минимальное наблюдение
    \item $\max$ - максимальное наблюдение
    \item $M$ - медиана
    \item $Q_1$ - первый квартиль (25\%)
    \item $Q_3$ - третий квартиль (75\%)
\end{itemize}

\emph{Межквартильный размах} задается разностью $\Delta Q = Q_3 - Q_1$.

Длина правого усика, это минимум между расстоянием от квартиля $Q_3$ до
максимального элемента и полутора полуторным межквартильным расстоянием.
Аналогично расчитывается длина правого усика.
\[l = \min(\max(x_i) - Q_3, 1.5 \Delta Q)\]
\[r = \min(Q_1 - \max(x_i), 1.5 \Delta Q)\]
Данные за усиками это \emph{выбросы}, а если они вышли за три межвартильных
расстояния это \emph{чрезвычайные выбросы}.

В ящике с усами используется медиана вместо матожидания\marginpar{Усеченное
    среднее. Если из выборки удалить 2,5\% самых маленьких наблюдений и 2,5\%
    самых больших, то выбросы уже не так сильно будут влиять на вычисленное
матожидание.} в качестве описания потому, что она устойчива к
выбросам.

\chapter{Кластеризация}

\section{Кластерный анализ}
\begin{marginfigure}
    \includegraphics[width=1.1\columnwidth]{pics/dendrogram.pdf}
    %\caption{Движение $y$}
    \label{fig:boxplot}
\end{marginfigure}
Расстояния между кластерами могут быть заданы по разному и стоит обращать
внимание на решаемую задачу. Чтобы задать расстояние между кластерами надо
определить расстояние между точками и на ее основе определить метод построения
кластерного расстояния. Чаще всего используют такие методы:

\begin{itemize}
    \item среднее невзвешенное расстояние (average linkage clustering) -
        среднее расстояние между парами из различных кластеров

    \item центроид метод (устаревший) - расстояние между кластерами
        определяется как расстояние между центрами масс.

    \item метод дальнего соседа (complete linked clustering) - по расстоянию
        между максимально удаленными точками кластеров

    \item метод ближайшего соседа (single linkage clustering) - по расстоянию
        между самыми близкими точками

    \item метод Варда (Ward's method)
\end{itemize}

Дендрограмма выращивается снизу вверх. Сперва каждая точка представляет свой
собственный кластер. Если при достижении уровня $h$ расстояние между точками
равно $h$ то точки объединяются в один кластер (на рисунке это спайка). Процесс
повторяется до тех пор пока не останется один единственный кластер.

Для определения количества кластеров используют метод \emph{плеча}. Для этого
строится график где по оси абсцисс отмечаются шаги объединения (первое
объединение, второе и так далее), по оси ординат откладывается высота на
котором произошло объединение кластеров. На графике ищется точка перелома,
когда расстояние резко увеличивается. Это количество кластеров в данных по
методу локтя.
\marginpar{Результаты кластерного анализа нужно интерпретировать: анализ всегда
должен давать что-то новое о данных, что общего у объектов в кластере и чем
различаются кластеры.}
\marginpar{
    Кривую локтя можно построить и для метода k-средних. В этом случае по оси
    абсцисс будет лежать выбранный $k$ по оси ординат качество получившейся
    кластеризации.

    См. \fullcite{yandex:book}, глава 7.
}

Недостатки иерархической кластеризации:
\begin{itemize}
    \item Плохо справляется с данными которые вытянуты в длинные ленты в
        простраснтве

    \item Для вычисления требуется хранить попарные расстояния между объектами.
\end{itemize}

\section{DBSCAN}
Density-based spatial clustering of applications with noise. Сильный
топологический алгоритм. Основан на поиске связных компонент в покрытии данных
$\epsilon$-шарами. Количество кластеров определяет автоматически.

Все объекты в наблюдениях делятся на три категории:
\begin{itemize}
    \item core point (внутренние / основные) - если в окрестности есть более
        $N_0$ соседей

    \item border point (граничные) - если в окрестности меньше $N_0$ внутренних

    \item noise point (шумовые) - если в окрестности нет внутренних точек.
        Автоматически содержат меньше $N_0$ объектов.
\end{itemize}

Алгоритм DBSCAN:
\begin{marginfigure}
    \includegraphics[width=1.3\columnwidth]{pics/dbscan.jpeg}
    %\caption{Движение $y$}
    \label{fig:boxplot}
\end{marginfigure}
\begin{enumerate}
    \item Шумовые точки удаляются из рассмотрения и не приписываются ни какому
        кластеру.

    \item Основные точки у которых есть общая окрестность соединяются ребром.
        Строится граф.

    \item В полученном графе вычисляются компоненты связности.

    \item Каждая граничная точка относится к тому кластеру, в который попала
        ближайшая к ней внутренняя точка.
\end{enumerate}

\chapter{Статистические гипотезы}

\section{Ошибки, значимость, p-value}

Если мы хотим опровергнуть какое-то утверждение, мы должны сформулировать
альтернативную гипотезу. Основная гипотеза всегда обозначается $H_0$,
альтернативная $H_1$. Основная гипотеза всегда должна быть простой, каким-то
конкретным утверждением, альтернативная может быть каким угодно. Такое
требование создается потому, что у нас нет мат.аппарата на сложные условия.

Если мы хотим проверить гипотезу, мы можем совершить такие ошибки:
\begin{table}
    \caption{Ошибки I-го и II-го рода}
    \label{tab:mistakes}
    \begin{center}
        \begin{tabular}[c]{c|c c}
             & гипотеза верна & гипотеза не верна \\
            % \multicolumn{1}{c|}{\textbf{}} & 
            % \multicolumn{1}{c|}{\textbf{гипотеза верна}} & 
            % \multicolumn{1}{c}{\textbf{гипотеза не верна}} \\
            \hline
            гипотеза принята & ok & \textbf{ошибка II рода} \\
            гипотеза отвергнута & {\it ошибка I рода} & ok \\
            
            % \hline
        \end{tabular}
    \end{center}
\end{table}

Влиять мы можем только на ошибку I-го рода. Ее обычно фиксируют на уровне 0,05,
0,01 или 0,005, но этот уровень можно менять на любой другой если этого требует
задача.

Ошибки второго рода контролировать сложнее. Для их уменьшения стараются
пользоваться состоятельными критериями.\marginpar{Критерий проверки гипотезы
называется состоятельным, если ошибка второго рода уменьшается с ростом числа
наблюдений.}

\begin{marginfigure}
    \includegraphics[width=1.1\columnwidth]{pics/critical_points_mistakes_value.pdf}
    \caption{Левое распределение соответствует гипотезе $H_0$, правое - гипотезе $H_1$.}
    \label{fig:crit_point}
\end{marginfigure}
Критическая точка критерия это такое значение вероятность превысить которое
равняется~$\alpha$ то есть вероятности ошибки первого рода. На рисунке
\ref{fig:crit_point} это площадь под графиком выше критической точки окрашенная
в красный. Синим обозначена вероятность ошибки II-го рода. Как видно это
вероятность получить выборку при которой не отвергается нулевая гипотеза, но
была верна альтернативная гипотеза.\marginpar{Проверять
    статистические гипотезы можно только в случае массовых явлений. События
    которые редки стат.гипотезами не проверяются. Число в 30 наблюдений
    считается достаточным только для нормальных распределений, для всех
    остальных надо больше данных.}

Критическая точка не обязана лежать на пересечении плотностей распределений
двух гипотез. Она определяется исходя только из плотности для нулевой гипотезы.

Альтернативой к механике проверки гипотез с критической точкой является
проверка гипотез через p-value. P-value вычисляется из предположения, что
нулевая гипотеза верна и определяет вероятность получить такое же значение
критерия или более экстремальное. Нам все равно нужно фиксировать уровень
значимости~$\alpha$ до эксперимента, но не нужно вычислять критическую точку.
Если p-value оказалось меньше~$\alpha$ то нулевая гипотеза должна быть
отвергнута, в противном случае нет оснований отвергать нулевую гипотезу.

\section{Нормальность}
Проверять данные на нормальность стоит не слишком тщательно. Данным достаточно только \emph{напоминать} нормальные, чтобы использовать большинство критериев в которых подразумевается, что данные нормальные. Если подходить к проверкам строго, то ничто и никогда невозможно будет проверить. 

Точно необходимо проверить:
\begin{enumerate}
    \item Выбросы. Отсутствие выбросов достаточно критичное условие. Если в данных есть выбросы, то их нельзя считать нормальными. С другой стороны если эти выбросы убрать, возможно уже можно будет работать.

    \item Асимметрия. Нормальные данные должны быть симметричными, поэтому если наблюдается асимметрия, то данные не нормальные. Возможно сможет помочь логарифмирование. 

    \item Отклонение от колоколообразности. Пограничным случаем колоколообразности можно считать равномерное распределение, а вот бимодальность гистограммы уже явный признак, что данные не нормальны. Еще чем больше наблюдений в данных, тем все-таки ближе должна быть гистограмма к нормальному распределению. За критерий можно взять выборку в $n=150$, чей график должен быть похож на колокол.
\end{enumerate}

Если в выборке менее 2000 объектов, то применяется метод Шапиро-Уилка, если же объектов больше, то используется критерий согласия Колмогорова-Смирнова.

\section{Сравнение мат.ожиданий}
Для сравнения мат.ожиданий используются:
\begin{itemize}
    \item t-критерий Стюдента (важно обращать внимание на равенство дисперсий)
    \item Fligner-Killeen test
    \item Brown-Forsythe test
\end{itemize}

\subsection{t-test, критерий Стьюдента}
Критерий Стюдента предполагает что метрика (н-р, матожидание) нормально распределена. Для двухвыборочного критерия важно равенство дисперсий. Так же тест не устойчив к выбросам в данных.

Одновыборочный тест\marginpar{
    Статистика для одновыборочного теста:\\
    $\overline x$ - выборочное матожидание \\
    $s^2$ - выборочная несмещенная оценка дисперсии \\
    $n$ - объем выборки
    \[H_0: M(x) = m\]
    \[t = \frac{\overline{x} - m}{s / \sqrt{n}}\]
    Степеней свободы $n-1$.

}
проводится когда у нас есть точное предположение о значении матожидания.

\paragraph{Двухвыборочный тест}
Предполагается, что есть две независимые выборки объема~$n_1$ и~$n_2$. Из
которых извлечены матожидания~$m_1$ и~$m_2$ и несмещенные дисперсии~$s_1^2$
и~$s_2^2$. 

\[H_0:m_1 = m_2\]

Если нулевая гипотеза верна, то разность $\Delta = m_1 - m_2$ имеет матожидание
$M(\Delta) = 0$\marginpar{
    Превращаем двувыборочный тест в одновыборочный!
}
и исходя из независимости выборок $D(\Delta) =
\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}$, так же получаем несмещенную
оценку разности выборочных средних $D(\Delta) = \frac{s_1^2}{n_1} +
\frac{s_2^2}{n_2}$. Теперь можем определить статистику

\begin{equation}
    \label{eq:t-test}
    t = \frac{m_1 - m_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
\end{equation}

Степени свободы вычисляются по формуле
\begin{equation}
    \label{eq:t-test-degree}
    df = \frac{
        \left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\right)^2
    }{
        (\frac{s_1^2}{n_1})^2/(n_1 - 1) + (\frac{s_2^2}{n_2})^2/(n_2-1)
    }
\end{equation}

\section{Сравнение медиан}
Для сравнения медиан используются:
\begin{itemize}
    \item Mood's median test. Только для гигантских данных. На малых данных будет большая ошибка 2 рода.
    \item критерий Мана-Уитни. Есть критика критерия, но крайние случаи довольно редкие и ими пренебрегают и все равно используют критерий.
\end{itemize}

\section{Проверка зависимости}
При проверке зависимости между двумя случайными величинами используют коэффициенты корреляции:
\begin{itemize}
    \setlength\itemsep{0em}
    \item Пирсона, если данные нормальные.
    \item Спирмена, в остальных случаях.
    \item Кендела, очень редко.
\end{itemize}

При вычислении корреляций надо понимать, что они ищут только линейные зависимости, коэффициенты могут быть очень чувствительными к выбросам и иногда могут показывать ложную корреляцию, например, для монотонных временных рядов.

\section{Линейная регрессия}
\begin{marginfigure}
    \includegraphics[width=1\columnwidth]{pics/anscombe_quartet.pdf}
    \caption{Квартет Анскомба. Иллюстрация как линейная регрессия может лажать. Для данных подобрали не ту модель или в данных есть сильные выбросы - все приводит к некорректному построению регрессии.}
    \label{fig:anscombe}
\end{marginfigure}

Для Определения на сколько модель хороша используют коэффициент детерминации. Чтобы его вычислить нужна обученная модель.
\begin{equation}
    \label{eq:coeff_of_determination}
    R^2 = 1 - \frac{D[y|x]}{D[y]}
\end{equation}
В сущностной части это на сколько дисперсия предсказания отличается от дисперсии наблюдений.

\marginpar{
    \emph{Наивная мультивариантная линейная регрессия:}
    \[ y = X\beta + \epsilon\]
    \[\epsilon \sim N(0,\, \sigma^2 I)\]
    $\epsilon$ - коэффициент \emph{невязки}, определяет шум системы.

    Коэффициенты находятся формулой
    \[ \hat \beta = (X^TX)^{-1}X^Ty \]

    Вариация вычисленных коэффициентов
    \[ Var(\hat \beta) = \sigma^2 (X^TX)^{-1}\]
    Это легко вывести если воспользоваться формулой дисперсии произведения 
    \[Var(AX) = A \cdot Var(X) \cdot A^T.\]
}

Во время построения модели регрессии самая большая проблема - наличие коллинеарных векторов (зависимых переменных в фичах). Чтобы очистить список фичей, можно проверить гипотезу что вычисленная фича нулевая (для каждой фичи индивидуально). 

\begin{equation*}
    \label{eq:zero_coeff_hyp}
    \left\{
    \begin{array}{lr}
        H_0: & \; \hat \beta_i = 0 \\
        H_1: & \; \hat \beta_i \ne 0
    \end{array}
    \right.
\end{equation*}

Если предположить что коэффициент имеет нормальное распределение, то мы можем
воспользоваться t-тестом Стьюдента и посчитать p-value. Если гипотеза не будет
отвергнута, это будет означать, что мы фичу стоит выкинуть потому что ее
коэффициент равен нулю. Соответственно там где гипотеза не подтвердилась фичи
являются ценными.

Выкидывать фичи стоит по одному. Начиная с самой слабой по p-value, каждый раз
с новым набором фичей анализ лучше проводить заново.\sidenote{\fullcite{Watts:ItS}}

\subsection{Для временных рядов}
Линейная регрессия может быть очень полезной во временных рядах, когда мы наблюдаем в них несколько сезонностей или когда данных относительно мало, нет нескольких полных сезонов в данных.

Чтобы учесть сезонность во временных рядах создают фичи:
\begin{itemize}
    \setlength\itemsep{0em}
    \item 1 - свободный коэффициент
    \item $\delta_s(t)$ - учет сезонности
\end{itemize}
При этом очевидно, что если сложить все фичи $\delta_s(t)$ сезонности мы получим свободную переменную, то есть на лицо линейная зависимость. 


\medskip
\printbibliography

\end{document}
